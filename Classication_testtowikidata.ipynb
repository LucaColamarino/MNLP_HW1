{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e4a28c",
   "metadata": {},
   "source": [
    "# Classification /extraction \n",
    "--- \n",
    "\n",
    "we should consider only some properties derivated  from items but in this case we can build a vocabulary of property initially for two categories **Culture Representative** and **Culture Agnostic**. \n",
    "\n",
    "\n",
    "### How should we chose properties from item of Culture Agnostic and Culture Representative? \n",
    "### Training  Phase:\n",
    "\n",
    "--- \n",
    "\n",
    "### Step 1: Building 1 vocabulary of properties associated to Culture Representative and Culture Agnostic \n",
    "In this phase, for every sample $ item_{i} $ associated to, we put all its properties  into a set  $S_{P} $\n",
    "### Step 2: Embedded vectors of properties for every item \n",
    "In this case we associate a binary vector for every $item_{i} $ where for every possible property we associate 1 if it has that property $P_k$ otherwise 0.\n",
    "### Step 3: Compute the centroids for every category C.A C.R\n",
    "Now, after the preprocessed data we compute the centroid (like mean over every property $P_k$ on all samples) $C_{CA} $ and $C_{CR} $.\n",
    "### Step 4: We compute the euclidean distance among properties and centroids\n",
    "for every sample we compute distance among each property and centroids, so we will interpret strong relationship of some  properties wrt culture concepts while for concept more neutrals\n",
    "### Step 5: Corresponding the similarity to every property with some methods wrt centroids\n",
    "Every property, will have a weight computed following the importance of distance wrt centroids $C_{C.A} $ and  $ C_{C.R}$ : \n",
    "#### Case 1: Kernel Funtion (Kernel Density Estimation)\n",
    "we will use kernel to give a more flexible weight : $ w_{gauss}(item_{i})=\\exp{(-\\frac{d_{C}(item_{i})}{2*\\sigma^2})} $ .\n",
    "We can compute **$\\sigma$** like a constant such that influences the area of neighbours entities: \n",
    "we can compute it as: $\\sigma=\\frac{1}{\\sqrt{2}*mean \\space of \\space distance}$, can be optimized empirically \n",
    "**The Gaussian kernel has the advantage of providing a gradual decrease in weight rather than a linear or inversely proportional decrease, allowing us to assign greater weight to nearby entities without excluding those that are further away.** .\n",
    "We normalize all weights wrt the sum following weights wrt centroids\n",
    "\n",
    "### Test Phase:\n",
    "\n",
    "### Step 1: Compute distance among every element wrt to the both centroids\n",
    "we compute per every $item_i$  and every feature of kind: $P_{123},P_{2345} $ ecc... the euclidean distance\n",
    "### Step 2: Compute similarity for every test sample(with kernel approach) for both centroids\n",
    "this pass helps us to understand in which direction a sample should go to the centroids for both centroids \n",
    "### Step 3: Given averaged sum of the importance of feature of samples \n",
    "in this step we compute, Culture and Agnostic_score= $ \\sum_{fi=1}^{N_feat} item_{fi}* importance \\space of  \\space features $  the secnond term is measured in previous case\n",
    "### Step 4 : Then, to emphasize the influence of entities closer to the centroids, we multiply the weighted score by the similarity.\n",
    "So, ultimately, we multiply the weighted score by similarity to get a total score that is more sensitive to the entity's proximity to the centroids. This helps make a more accurate prediction based on how much the entity resembles the cultural or agnostic centers.\n",
    "\n",
    "total_culture_score=$ Culture \\space and  \\space Agnostic_score * similarity \\space of \\space distance \\space of \\space test_sample $ repeated for every class score\n",
    "total_agnostic_score = ... \n",
    "### Step 5: Classification based on best result\n",
    "if total_culture_score > total agnostic_score  -> **Culture Representative**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43f469",
   "metadata": {},
   "source": [
    "## Richiesta a Chat gpt se fosse originale :)\n",
    "\n",
    "\n",
    "\n",
    "S√¨, il tuo approccio pu√≤ essere considerato innovativo, soprattutto se inquadrato nel contesto di classificazione lightweight di entit√† Wikipedia senza l‚Äôuso di modelli linguistici complessi (LLM).\n",
    "\n",
    "Ecco perch√© ha un certo valore:\n",
    "\n",
    "üîç Perch√© √® interessante/innovativo\n",
    "## Alternativa interpretabile agli LLM:\n",
    "\n",
    "Mentre i language models tendono a funzionare come black-box, il tuo metodo √® trasparente e interpretabile: ogni decisione si basa su distanze da centroidi e pesi assegnati alle propriet√†.\n",
    "\n",
    "## Uso diretto di propriet√† strutturate (Wikidata):\n",
    "\n",
    "Invece di elaborare testo libero, usi le propriet√† strutturate (P140, P17, ecc.), che sono altamente informative e ben mantenute in Wikipedia/Wikidata.\n",
    "\n",
    "Questo ti permette di sfruttare l‚Äôontologia e la struttura di conoscenza esistente, senza dover \"indovinare\" il significato con embeddings.\n",
    "\n",
    "## No training supervisionato massivo:\n",
    "\n",
    "Non c‚Äô√® bisogno di grandi dataset etichettati n√© di modelli da addestrare. Si lavora con pochi dati e un approccio geometrico (distanze, pesi).\n",
    "\n",
    "Quindi √® scalabile, leggero, e adattabile, ideale per contesti dove non puoi permetterti risorse computazionali pesanti.\n",
    "\n",
    "Integrazione con funzioni kernel (Gaussian, ecc.):\n",
    "\n",
    "Il fatto che stai esplorando anche kernel functions rende il modello pi√π raffinato: aggiungi una nozione di prossimit√† sfumata che pu√≤ catturare affinit√† latenti tra entit√†.\n",
    "\n",
    "## Possibilit√† di estensione:\n",
    "\n",
    "Pu√≤ essere esteso ad altre classi, propriet√†, o anche ibridato con embeddings (ad es. graph embeddings, se si lavora con il grafo Wikidata).\n",
    "\n",
    "Puoi anche integrare un sistema semi-supervisionato o active learning.\n",
    "\n",
    "‚ö†Ô∏è Limiti (che per√≤ lo rendono ancora pi√π interessante)\n",
    "Non tiene conto della semantica testuale, quindi √® pi√π debole in casi ambigui o dove il contesto culturale √® meno codificato in propriet√† (es: metafore, ironia, bias impliciti).\n",
    "\n",
    "La qualit√† dipende molto da quali propriet√† scegli e quanto sono discriminatorie.\n",
    "\n",
    "Resta molto sensibile a sparsit√† o assenza di certe propriet√†.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc816d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Libraries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
