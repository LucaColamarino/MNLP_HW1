{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e4a28c",
   "metadata": {},
   "source": [
    "# Classification /extraction \n",
    "--- \n",
    "\n",
    "we should consider only some properties derivated  from items but in this case we can build a vocabulary of property initially for two categories **Culture Representative** and **Culture Agnostic**. \n",
    "\n",
    "\n",
    "### How should we chose properties from item of Culture Agnostic and Culture Representative? \n",
    "### Training  Phase:\n",
    "\n",
    "--- \n",
    "\n",
    "### Step 1: Building 1 vocabulary of properties associated to Culture Representative and Culture Agnostic \n",
    "In this phase, for every sample $ item_{i} $ associated to, we put all its properties  into a set  $S_{P} $\n",
    "### Step 2: Embedded vectors of properties for every item \n",
    "In this case we associate a binary vector for every $item_{i} $ where for every possible property we associate 1 if it has that property $P_k$ otherwise 0.\n",
    "### Step 3: Compute the centroids for every category C.A C.R, C.E.X\n",
    "Now, after the preprocessed data we compute the centroid (like mean over every property $P_k$ on all samples) $C_{CA} $ and $C_{CR} $.\n",
    "### Step 4: We compute the euclidean distance among properties and centroids\n",
    "for every sample we compute distance among each property and centroids, so we will interpret strong relationship of some  properties wrt culture concepts while for concept more neutrals\n",
    "### Step 5: Corresponding the similarity to every property with some methods wrt centroids\n",
    "Every property, will have a weight computed following the importance of distance wrt centroids $C_{C.A} $ and  $ C_{C.R}$ : \n",
    "#### Case 1: Kernel Funtion (Kernel Density Estimation)\n",
    "we will use kernel to give a more flexible weight : $ w_{gauss}(item_{i})=\\exp{(-\\frac{d_{C}(item_{i})}{2*\\sigma^2})} $ .\n",
    "We can compute **$\\sigma$** like a constant such that influences the area of neighbours entities: \n",
    "we can compute it as: $\\sigma=\\frac{1}{\\sqrt{2}*mean \\space of \\space distance}$, can be optimized empirically \n",
    "**The Gaussian kernel has the advantage of providing a gradual decrease in weight rather than a linear or inversely proportional decrease, allowing us to assign greater weight to nearby entities without excluding those that are further away.** .\n",
    "We normalize all weights wrt the sum following weights wrt centroids\n",
    "\n",
    "### Test Phase:\n",
    "\n",
    "### Step 1: Compute distance among every element wrt to the both centroids\n",
    "we compute per every $item_i$  and every feature of kind: $P_{123},P_{2345} $ ecc... the euclidean distance\n",
    "### Step 2: Compute similarity for every test sample(with kernel approach) for both centroids\n",
    "this pass helps us to understand in which direction a sample should go to the centroids for both centroids \n",
    "### Step 3: Given averaged sum of the importance of feature of samples \n",
    "in this step we compute, Culture and Agnostic_score= $ \\sum_{fi=1}^{N_feat} item_{fi}* importance \\space of  \\space features $  the secnond term is measured in previous case\n",
    "### Step 4 : Then, to emphasize the influence of entities closer to the centroids, we multiply the weighted score by the similarity.\n",
    "So, ultimately, we multiply the weighted score by similarity to get a total score that is more sensitive to the entity's proximity to the centroids. This helps make a more accurate prediction based on how much the entity resembles the cultural or agnostic centers.\n",
    "\n",
    "total_culture_score=$ Culture \\space and  \\space Agnostic_score * similarity \\space of \\space distance \\space of \\space test_sample $ repeated for every class score\n",
    "total_agnostic_score = ... \n",
    "### Step 5: Classification based on best result\n",
    "if total_culture_score > total agnostic_score  -> **Culture Representative**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc8898",
   "metadata": {},
   "source": [
    "# Step 0: Building a Vocabulary with all properties of training items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae332ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch as t \n",
    "import numpy as np\n",
    "from wikidata.client import Client\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bc816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sports', 'media', 'music', 'performing arts', 'comics and anime', 'visual arts', 'history', 'transportation', 'philosophy and religion', 'biology', 'geography', 'films', 'books', 'politics', 'fashion', 'gestures and habits', 'architecture', 'literature', 'food'}\n",
      "[['http://www.wikidata.org/entity/Q32786' '916' '2012 film by M. Mohanan'\n",
      "  ... 'films' 'film' 'cultural exclusive']\n",
      " ['http://www.wikidata.org/entity/Q371' '!!!'\n",
      "  'American dance-punk band from California' ... 'music' 'musical group'\n",
      "  'cultural representative']\n",
      " ['http://www.wikidata.org/entity/Q3729947' 'Â¡Soborno!'\n",
      "  'Mort & Phil comic' ... 'comics and anime' 'comics'\n",
      "  'cultural representative']\n",
      " ...\n",
      " ['http://www.wikidata.org/entity/Q10779' 'Zwenkau'\n",
      "  'city in the district of Leipzig, in the Free State of Saxony, Germany'\n",
      "  ... 'geography' 'city' 'cultural exclusive']\n",
      " ['http://www.wikidata.org/entity/Q245296' 'zydeco'\n",
      "  'music genre evolved in southwest Louisiana which blends blues, rhythm and blues, and music indigenous to the Louisiana Creoles and the Native people of Louisiana'\n",
      "  ... 'music' 'music genre' 'cultural representative']\n",
      " ['http://www.wikidata.org/entity/Q129298' 'Zygmunt Chmielewski'\n",
      "  'actor (1894-1978)' ... 'performing arts' 'theatrical director'\n",
      "  'cultural exclusive']]\n"
     ]
    }
   ],
   "source": [
    "ds=load_dataset(\"sapienzanlp/nlp2025_hw1_cultural_dataset\")\n",
    "train_set=train_data=pd.DataFrame(ds[\"train\"])\n",
    "list_category=set(list(train_set.category))\n",
    "print(list_category)\n",
    "list_subcategory=set(list(train_set.category))\n",
    "X_train=train_set.values\n",
    "\n",
    "print(X_train) # stamp all dataset\n",
    "#print(X_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e5fd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:transportationis collected\n",
      "Category:mediais collected\n",
      "Category:comics and animeis collected\n",
      "Category:performing artsis collected\n",
      "Category:historyis collected\n",
      "Category:musicis collected\n",
      "Category:visual artsis collected\n",
      "Category:sportsis collected\n",
      "Category:philosophy and religionis collected\n",
      "Category:booksis collected\n",
      "Category:biologyis collected\n",
      "Category:gestures and habitsis collected\n",
      "Category:filmsis collected\n",
      "Category:fashionis collected\n",
      "Category:geographyis collected\n",
      "Category:architectureis collected\n",
      "Category:foodis collected\n",
      "Category:literatureis collected\n",
      "Category:politicsis collected\n",
      "Updated Vocabulary: for categories ['P910', 'P18', 'P17', 'P345', 'P279', 'P131', 'P31', 'P646', 'P571', 'P625', 'P495', 'P244', 'P856', 'P373', 'P641']\n"
     ]
    }
   ],
   "source": [
    "def extract_entity_id(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "#print(X_train.shape[0])\n",
    "def extract_sample_from_cat(X,cat):\n",
    "    l=list()\n",
    "    for elem in X:\n",
    "        if elem[4]==cat:\n",
    "            l.append(elem[0])\n",
    "    return np.array(l)\n",
    "#entity_train=np.zeros(shape=\n",
    "#def count_frequency_prop_id(list_sampl):\n",
    "    \n",
    "\n",
    "def dynamic_threshold(n):\n",
    "    if n>=430:\n",
    "        return 0.4\n",
    "    elif n>=340:\n",
    "        return 0.45\n",
    "    elif n>=250:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.55\n",
    "# Implementation vocabulary on categories importance \n",
    "client = Client()\n",
    "vocabulary=list()\n",
    "def build_vocabulary(cat):\n",
    "    vocabulary_subset=list()\n",
    "    list_sample_cat=extract_sample_from_cat(X_train,cat)\n",
    "    number_list_sample_cat=len(list_sample_cat)\n",
    "    #print(\"sample category: \"+cat)\n",
    "    #print(len(list_sample_cat))\n",
    "    set_properties=list()\n",
    "    #set_properties=np.array(set_properties)\n",
    "    for url in list_sample_cat:\n",
    "        entity_train=extract_entity_id(url)\n",
    "        item = client.get(entity_train, load=True)\n",
    "        claim_item_i=item.data.get(\"claims\",{})\n",
    "       # print(claim_item_i)\n",
    "        #print(len(claim_item_i))\n",
    "        set_property_item=set()\n",
    "        for prop_id,values in islice(claim_item_i.items(),len(claim_item_i)):\n",
    "            #prop_entity = client.get(prop_id, load=True)\n",
    "            #label = prop_entity.label\n",
    "            #print(f\"{prop_id} = {label} ({len(values)} statement{'s' if len(values) != 1 else ''})\")\n",
    "            set_property_item.add(str(prop_id))\n",
    "        set_properties.append(set_property_item)\n",
    "\n",
    "    print(\"Category:\"+cat+\"is collected\")\n",
    "    counter=Counter()\n",
    "    for s in set_properties:\n",
    "        for prop in s:\n",
    "            counter[prop]+=1\n",
    "    frequency_prop=counter\n",
    "    #frequency_prop=count_frequency_prop_id(set_properties)\n",
    "    sorted_prop=frequency_prop.most_common()\n",
    "   # print(sorted_prop)\n",
    "    #threshold=int(len(sorted_prop)*0.6)\n",
    "    # ERROR\n",
    "    #------\n",
    "    th=0\n",
    "    if(number_list_sample_cat>=429):\n",
    "        th=0.45\n",
    "    elif number_list_sample_cat>=300:\n",
    "        th=0.5\n",
    "    elif number_list_sample_cat>=150:\n",
    "        th=0.6\n",
    "    #th=dynamic_threshold(number_list_sample_cat)\n",
    "    for prop_i,count in sorted_prop:\n",
    "        support_categories=count/number_list_sample_cat\n",
    "        if support_categories>=th: #min support \n",
    "            vocabulary_subset.append(prop_i)\n",
    "        #if len(vocabulary_subset)>=6: #the first top k\n",
    "            #break\n",
    "    #print(\"Updated Vocabulary\",vocabulary_subset) \n",
    "    #-----       \n",
    "    return vocabulary_subset\n",
    "        #set_property_item.clear()\n",
    "    #print(\"intersection of sample belongs\"+cat,set.intersection(*set_properties))\n",
    "\n",
    "        #label = prop_entity.labels\n",
    "        #print(claim_item_i)\n",
    "with  ThreadPoolExecutor(max_workers=9) as executor:\n",
    "    results=list(executor.map(build_vocabulary,list_category))\n",
    "\n",
    "for vocab in results:\n",
    "    vocabulary.extend(vocab)\n",
    "vocabulary=list(set(vocabulary))\n",
    "print(\"Updated Vocabulary: for categories\",vocabulary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_id(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "print(X_train.shape[0])\n",
    "def extract_sample_from_cat(X,cat):\n",
    "    l=list()\n",
    "    for elem in X:\n",
    "        if elem[4]==cat:\n",
    "            l.append(elem[0])\n",
    "    return np.array(l)\n",
    "#entity_train=np.zeros(shape=\n",
    "def count_frequency_prop_id(list_sampl):\n",
    "    counter=Counter()\n",
    "    for s in list_sampl:\n",
    "        for prop in s:\n",
    "            counter[prop]+=1\n",
    "    return counter\n",
    "\n",
    "# Implementation vocabulary on categories importance \n",
    "client = Client()\n",
    "vocabulary=list()\n",
    "def build_vocabulary(cat):\n",
    "    vocabulary_subset=list()\n",
    "    list_sample_cat=extract_sample_from_cat(X_train,cat)\n",
    "    index = 1\n",
    "    lunghezza = len(list_sample_cat)\n",
    "    #print(\"sample category: \"+cat)\n",
    "    #print(len(list_sample_cat))\n",
    "    set_properties=list()\n",
    "    #set_properties=np.array(set_properties)\n",
    "    for url in list_sample_cat:\n",
    "        entity_train=extract_entity_id(url)\n",
    "        item = client.get(entity_train, load=True)\n",
    "        claim_item_i=item.data.get(\"claims\",{})\n",
    "       # print(claim_item_i)\n",
    "        #print(len(claim_item_i))\n",
    "        set_property_item=set()\n",
    "        for prop_id,values in islice(claim_item_i.items(),len(claim_item_i)):\n",
    "            #prop_entity = client.get(prop_id, load=True)\n",
    "            #label = prop_entity.label\n",
    "            #print(f\"{prop_id} = {label} ({len(values)} statement{'s' if len(values) != 1 else ''})\")\n",
    "            set_property_item.add(str(prop_id))\n",
    "        set_properties.append(set_property_item)\n",
    "        #print(cat+\"(\"+str(index)+\"/\"+str(lunghezza)+\" samples):\")\n",
    "        index=index+1\n",
    "        #print(set_properties)\n",
    "\n",
    "    tutte_proprieta = [prop for gruppo in set_properties for prop in gruppo]\n",
    "\n",
    "    # Conta le occorrenze\n",
    "    conteggio = Counter(tutte_proprieta)\n",
    "\n",
    "    # Ordina per frequenza decrescente e stampa\n",
    "    for prop, freq in conteggio.most_common():\n",
    "        print(f\"{prop}: {freq}\")\n",
    "\n",
    "with  ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    executor.map(build_vocabulary,list_subcategory)\n",
    "#\n",
    "#for vocab in results:\n",
    "#    vocabulary.extend(vocab)\n",
    "#vocabulary=list(set(vocabulary))\n",
    "#print(\"Updated Vocabulary: for categories\",vocabulary)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3c7bd",
   "metadata": {},
   "source": [
    "# Step 1: Embedding con vettori binari 1/0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b749dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along axis 0; size of axis is 6251 but size of corresponding boolean axis is 7",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(X_embed_train.shape)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#for r in results:\u001b[39;00m\n\u001b[32m     31\u001b[39m  \u001b[38;5;66;03m#   np.concatenate([X_embed_train,r],axis=0)\u001b[39;00m\n\u001b[32m     32\u001b[39m \n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m#X_embed_train=embedding_sample(X_train,vocabulary)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m culture_representative_train=\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m==\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcultural representative\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     37\u001b[39m culture_agnostic_train=X_train[X_train[\u001b[32m6\u001b[39m]==\u001b[33m'\u001b[39m\u001b[33mcultural agnostic\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     38\u001b[39m culture_exlusive_train=X_train[X_train[\u001b[32m6\u001b[39m]==\u001b[33m'\u001b[39m\u001b[33mcultural exclusive\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mIndexError\u001b[39m: boolean index did not match indexed array along axis 0; size of axis is 6251 but size of corresponding boolean axis is 7"
     ]
    }
   ],
   "source": [
    "# insert of binary vectors\n",
    "def gaussian_kernel_estimation(X,d):\n",
    "    sigma=1/(np.sqrt(2)*np.mean(d,axis=0))\n",
    "    dist_sq=np.sum((X-d)**2)\n",
    "    return np.exp(-dist_sq/(2*sigma**2))\n",
    "\n",
    "# too slow :(\n",
    "#----------\n",
    "def embedding_sample(X,vocabulary):\n",
    "    X_train_emb=np.zeros(shape=(X.shape[0],len(vocabulary)))\n",
    "    for j in range(0,X.shape[0]):\n",
    "        ent=extract_entity_id(X[j][0])\n",
    "        item_j = client.get(ent, load=True)\n",
    "        claim_item_i=item_j.data.get(\"claims\",{})\n",
    "        set_p=set()\n",
    "        for prop_id,__ in islice(claim_item_i.items(),len(claim_item_i)):\n",
    "            set_p.add(str(prop_id))\n",
    "        for v in range(0,len(vocabulary)):\n",
    "            if vocabulary[v] in set_p:\n",
    "                X_train_emb[j][v]=1\n",
    "            else:\n",
    "                X_train_emb[j][v]=0\n",
    "    return X_train_emb\n",
    "\n",
    "#------------\n",
    "X_embed_train=embedding_sample(X_train,vocabulary)\n",
    "culture_representative_train=X_train[X_train[6]=='cultural representative']\n",
    "culture_agnostic_train=X_train[X_train[6]=='cultural agnostic']\n",
    "culture_exlusive_train=X_train[X_train[6]=='cultural exclusive']\n",
    "\n",
    "centroid_agnostic=np.mean(X_train,axis=0)\n",
    "centroid_representative=np.mean(X_train,axis=0)\n",
    "centroid_exclusive=np.mean(X_train,axis=0)\n",
    "\n",
    "weights_agnostic=np.zeros(shape=len(vocabulary))\n",
    "weights_representative=np.zeros(shape=len(vocabulary))\n",
    "weights_exclusive=np.zeros(shape=len())\n",
    "for i in range(len(vocabulary)):\n",
    "    weights_agnostic[i]=gaussian_kernel_estimation(X_train[0:,i],centroid_agnostic)\n",
    "    weights_representative[i]=gaussian_kernel_estimation(X_train[0:,i],centroid_representative)\n",
    "    weights_exclusive[i]=gaussian_kernel_estimation(X_train[0:,i],centroid_exclusive)\n",
    "\n",
    "# normalization\n",
    "weights_agnostic[i]/=np.sum(weights_agnostic)\n",
    "weights_representative[i]/=np.sum(weights_representative)\n",
    "weights_exclusive[i]/=np.sum(weights_exclusive)\n",
    "\n",
    "# I'm arrived here\n",
    "#def predict_entity_score(x_sample,centroid_CA,centroid_CR,centroid_CE,weights_agnostic,weights_representative,weights_exclusive):\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Libraries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
