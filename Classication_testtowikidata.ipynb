{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e4a28c",
   "metadata": {},
   "source": [
    "# Classification /extraction \n",
    "--- \n",
    "\n",
    "we should consider only some properties derivated  from items but in this case we can build a vocabulary of property initially for two categories **Culture Representative** and **Culture Agnostic**. \n",
    "\n",
    "\n",
    "### How should we chose properties from item of Culture Agnostic and Culture Representative? \n",
    "### Training  Phase:\n",
    "\n",
    "--- \n",
    "\n",
    "### Step 1: Building 1 vocabulary of properties associated to Culture Representative and Culture Agnostic \n",
    "In this phase, for every sample $ item_{i} $ associated to, we put all its properties  into a set  $S_{P} $\n",
    "### Step 2: Embedded vectors of properties for every item \n",
    "In this case we associate a binary vector for every $item_{i} $ where for every possible property we associate 1 if it has that property $P_k$ otherwise 0.\n",
    "### Step 3: Compute the centroids for every category C.A C.R, C.E.X\n",
    "Now, after the preprocessed data we compute the centroid (like mean over every property $P_k$ on all samples) $C_{CA} $ and $C_{CR} $.\n",
    "### Step 4: We compute the euclidean distance among properties and centroids\n",
    "for every sample we compute distance among each property and centroids, so we will interpret strong relationship of some  properties wrt culture concepts while for concept more neutrals\n",
    "### Step 5: Corresponding the similarity to every property with some methods wrt centroids\n",
    "Every property, will have a weight computed following the importance of distance wrt centroids $C_{C.A} $ and  $ C_{C.R}$ : \n",
    "#### Case 1: Kernel Funtion (Kernel Density Estimation)\n",
    "we will use kernel to give a more flexible weight : $ w_{gauss}(item_{i})=\\exp{(-\\frac{d_{C}(item_{i})}{2*\\sigma^2})} $ .\n",
    "We can compute **$\\sigma$** like a constant such that influences the area of neighbours entities: \n",
    "we can compute it as: $\\sigma=\\frac{1}{\\sqrt{2}*mean \\space of \\space distance}$, can be optimized empirically \n",
    "**The Gaussian kernel has the advantage of providing a gradual decrease in weight rather than a linear or inversely proportional decrease, allowing us to assign greater weight to nearby entities without excluding those that are further away.** .\n",
    "We normalize all weights wrt the sum following weights wrt centroids\n",
    "\n",
    "### Test Phase:\n",
    "\n",
    "### Step 1: Compute distance among every element wrt to the both centroids\n",
    "we compute per every $item_i$  and every feature of kind: $P_{123},P_{2345} $ ecc... the euclidean distance\n",
    "### Step 2: Compute similarity for every test sample(with kernel approach) for both centroids\n",
    "this pass helps us to understand in which direction a sample should go to the centroids for both centroids \n",
    "### Step 3: Given averaged sum of the importance of feature of samples \n",
    "in this step we compute, Culture and Agnostic_score= $ \\sum_{fi=1}^{N_feat} item_{fi}* importance \\space of  \\space features $  the secnond term is measured in previous case\n",
    "### Step 4 : Then, to emphasize the influence of entities closer to the centroids, we multiply the weighted score by the similarity.\n",
    "So, ultimately, we multiply the weighted score by similarity to get a total score that is more sensitive to the entity's proximity to the centroids. This helps make a more accurate prediction based on how much the entity resembles the cultural or agnostic centers.\n",
    "\n",
    "total_culture_score=$ Culture \\space and  \\space Agnostic_score * similarity \\space of \\space distance \\space of \\space test_sample $ repeated for every class score\n",
    "total_agnostic_score = ... \n",
    "### Step 5: Classification based on best result\n",
    "if total_culture_score > total agnostic_score  -> **Culture Representative**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc8898",
   "metadata": {},
   "source": [
    "# Step 0: Building a Vocabulary with all properties of training items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae332ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch as t \n",
    "import numpy as np\n",
    "from wikidata.client import Client\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from scipy.spatial.distance import euclidean\n",
    "import urllib.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44bc816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'music', 'media', 'politics', 'books', 'architecture', 'visual arts', 'literature', 'philosophy and religion', 'food', 'biology', 'history', 'performing arts', 'geography', 'sports', 'transportation', 'comics and anime', 'fashion', 'gestures and habits', 'films'}\n",
      "[['http://www.wikidata.org/entity/Q32786' '916' '2012 film by M. Mohanan'\n",
      "  ... 'films' 'film' 'cultural exclusive']\n",
      " ['http://www.wikidata.org/entity/Q371' '!!!'\n",
      "  'American dance-punk band from California' ... 'music' 'musical group'\n",
      "  'cultural representative']\n",
      " ['http://www.wikidata.org/entity/Q3729947' 'Â¡Soborno!'\n",
      "  'Mort & Phil comic' ... 'comics and anime' 'comics'\n",
      "  'cultural representative']\n",
      " ...\n",
      " ['http://www.wikidata.org/entity/Q10779' 'Zwenkau'\n",
      "  'city in the district of Leipzig, in the Free State of Saxony, Germany'\n",
      "  ... 'geography' 'city' 'cultural exclusive']\n",
      " ['http://www.wikidata.org/entity/Q245296' 'zydeco'\n",
      "  'music genre evolved in southwest Louisiana which blends blues, rhythm and blues, and music indigenous to the Louisiana Creoles and the Native people of Louisiana'\n",
      "  ... 'music' 'music genre' 'cultural representative']\n",
      " ['http://www.wikidata.org/entity/Q129298' 'Zygmunt Chmielewski'\n",
      "  'actor (1894-1978)' ... 'performing arts' 'theatrical director'\n",
      "  'cultural exclusive']]\n"
     ]
    }
   ],
   "source": [
    "ds=load_dataset(\"sapienzanlp/nlp2025_hw1_cultural_dataset\")\n",
    "train_set=train_data=pd.DataFrame(ds[\"train\"])\n",
    "list_category=set(list(train_set.category))\n",
    "print(list_category)\n",
    "list_subcategory=set(list(train_set.category))\n",
    "X_train=train_set.values\n",
    "\n",
    "print(X_train) # stamp all dataset\n",
    "#print(X_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5fd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:booksis collected\n",
      "Category:mediais collected\n",
      "Category:foodis collected\n",
      "Category:biologyis collected\n",
      "Category:visual artsis collected\n",
      "Category:musicis collected\n",
      "Category:architectureis collected\n",
      "Category:literatureis collected\n",
      "Category:philosophy and religionis collected\n",
      "Category:historyis collected\n",
      "Category:politicsis collected\n",
      "Category:transportationis collected\n",
      "Category:performing artsis collected\n",
      "Category:gestures and habitsis collected\n",
      "Category:comics and animeis collected\n",
      "Category:fashionis collected\n",
      "Category:geographyis collected\n",
      "Category:sportsis collected\n",
      "Category:filmsis collected\n",
      "Updated Vocabulary: for categories ['P1566', 'P214', 'P646', 'P18', 'P373', 'P910', 'P244', 'P159', 'P27', 'P6366', 'P227', 'P345', 'P279', 'P106', 'P856', 'P8189', 'P31', 'P131', 'P495', 'P19', 'P569', 'P21', 'P571', 'P641', 'P625', 'P17']\n"
     ]
    }
   ],
   "source": [
    "def extract_entity_id(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "#print(X_train.shape[0])\n",
    "def extract_sample_from_cat(X,cat):\n",
    "    l=list()\n",
    "    for elem in X:\n",
    "        if elem[4]==cat:\n",
    "            l.append(elem[0])\n",
    "    return np.array(l)\n",
    "#entity_train=np.zeros(shape=\n",
    "#def count_frequency_prop_id(list_sampl):\n",
    "    \n",
    "\n",
    "def dynamic_threshold(n):\n",
    "    if n>=430:\n",
    "        return 0.35\n",
    "    elif n>=340:\n",
    "        return 0.4\n",
    "    elif n>=250:\n",
    "        return 0.45\n",
    "    else:\n",
    "        return 0.50\n",
    "# Implementation vocabulary on categories importance \n",
    "client = Client()\n",
    "vocabulary=list()\n",
    "def build_vocabulary(cat):\n",
    "    vocabulary_subset=list()\n",
    "    list_sample_cat=extract_sample_from_cat(X_train,cat)\n",
    "    number_list_sample_cat=len(list_sample_cat)\n",
    "    #print(\"sample category: \"+cat)\n",
    "    #print(len(list_sample_cat))\n",
    "    set_properties=list()\n",
    "    #set_properties=np.array(set_properties)\n",
    "    for url in list_sample_cat:\n",
    "        entity_train=extract_entity_id(url)\n",
    "        #-----Check on entity train\n",
    "        if not entity_train or not entity_train.startswith(\"Q\"): #verify if entity_id starts with Q\n",
    "            continue\n",
    "        try:\n",
    "            item = client.get(entity_train, load=True)\n",
    "        except urllib.error.HTTPError: #handle error of HTTP error like HTTP 404\n",
    "            continue\n",
    "        #------\n",
    "        claim_item_i=item.data.get(\"claims\",{})\n",
    "       # print(claim_item_i)\n",
    "        #print(len(claim_item_i))\n",
    "        set_property_item=set()\n",
    "        for prop_id,values in islice(claim_item_i.items(),len(claim_item_i)):\n",
    "            #prop_entity = client.get(prop_id, load=True)\n",
    "            #label = prop_entity.label\n",
    "            #print(f\"{prop_id} = {label} ({len(values)} statement{'s' if len(values) != 1 else ''})\")\n",
    "            set_property_item.add(str(prop_id))\n",
    "        set_properties.append(set_property_item)\n",
    "\n",
    "    print(\"Category:\"+cat+\" collected\")\n",
    "    counter=Counter()\n",
    "    for s in set_properties:\n",
    "        for prop in s:\n",
    "            counter[prop]+=1\n",
    "    frequency_prop=counter\n",
    "    #frequency_prop=count_frequency_prop_id(set_properties)\n",
    "    sorted_prop=frequency_prop.most_common()\n",
    "   # print(sorted_prop)\n",
    "    #threshold=int(len(sorted_prop)*0.6)\n",
    "    # ERROR\n",
    "    #------\n",
    "    th=0\n",
    "    if(number_list_sample_cat>=429):\n",
    "        th=0.35\n",
    "    elif number_list_sample_cat>=300:\n",
    "        th=0.4\n",
    "    elif number_list_sample_cat>=150:\n",
    "        th=0.45\n",
    "    #th=dynamic_threshold(number_list_sample_cat)\n",
    "    for prop_i,count in sorted_prop:\n",
    "        support_categories=count/number_list_sample_cat\n",
    "        if support_categories>=th: #min support \n",
    "            vocabulary_subset.append(prop_i)\n",
    "        #if len(vocabulary_subset)>=6: #the first top k\n",
    "            #break\n",
    "    #print(\"Updated Vocabulary\",vocabulary_subset) \n",
    "    #-----       \n",
    "    return vocabulary_subset\n",
    "        #set_property_item.clear()\n",
    "    #print(\"intersection of sample belongs\"+cat,set.intersection(*set_properties))\n",
    "\n",
    "        #label = prop_entity.labels\n",
    "        #print(claim_item_i)\n",
    "with  ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results=list(executor.map(build_vocabulary,list_category))\n",
    "\n",
    "for vocab in results:\n",
    "    vocabulary.extend(vocab)\n",
    "vocabulary=list(set(vocabulary))\n",
    "print(\"Updated Vocabulary: for categories\",vocabulary)\n",
    "#Updated Vocabulary: for categories ['P495', 'P279', 'P345', 'P910', 'P17', 'P571', 'P373', 'P18', 'P625', 'P856', 'P244', 'P646', 'P31', 'P131', 'P641']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_id(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "print(X_train.shape[0])\n",
    "def extract_sample_from_cat(X,cat):\n",
    "    l=list()\n",
    "    for elem in X:\n",
    "        if elem[4]==cat:\n",
    "            l.append(elem[0])\n",
    "    return np.array(l)\n",
    "#entity_train=np.zeros(shape=\n",
    "def count_frequency_prop_id(list_sampl):\n",
    "    counter=Counter()\n",
    "    for s in list_sampl:\n",
    "        for prop in s:\n",
    "            counter[prop]+=1\n",
    "    return counter\n",
    "\n",
    "# Implementation vocabulary on categories importance \n",
    "client = Client()\n",
    "vocabulary_subcategory=list()\n",
    "def build_vocabulary(cat):\n",
    "    vocabulary_subset=list()\n",
    "    list_sample_cat=extract_sample_from_cat(X_train,cat)\n",
    "    index = 1\n",
    "    lunghezza = len(list_sample_cat)\n",
    "    #print(\"sample category: \"+cat)\n",
    "    #print(len(list_sample_cat))\n",
    "    set_properties=list()\n",
    "    #set_properties=np.array(set_properties)\n",
    "    for url in list_sample_cat:\n",
    "        entity_train=extract_entity_id(url)\n",
    "        item = client.get(entity_train, load=True)\n",
    "        claim_item_i=item.data.get(\"claims\",{})\n",
    "       # print(claim_item_i)\n",
    "        #print(len(claim_item_i))\n",
    "        set_property_item=set()\n",
    "        for prop_id,values in islice(claim_item_i.items(),len(claim_item_i)):\n",
    "            #prop_entity = client.get(prop_id, load=True)\n",
    "            #label = prop_entity.label\n",
    "            #print(f\"{prop_id} = {label} ({len(values)} statement{'s' if len(values) != 1 else ''})\")\n",
    "            set_property_item.add(str(prop_id))\n",
    "        set_properties.append(set_property_item)\n",
    "        #print(cat+\"(\"+str(index)+\"/\"+str(lunghezza)+\" samples):\")\n",
    "        index=index+1\n",
    "        #print(set_properties)\n",
    "\n",
    "    tutte_proprieta = [prop for gruppo in set_properties for prop in gruppo]\n",
    "\n",
    "    # Conta le occorrenze\n",
    "    conteggio = Counter(tutte_proprieta)\n",
    "\n",
    "    # Ordina per frequenza decrescente e stampa\n",
    "    for prop, freq in conteggio.most_common():\n",
    "        print(f\"{prop}: {freq}\")\n",
    "\n",
    "with  ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results_subcategory=list(executor.map(build_vocabulary,list_subcategory))\n",
    "#\n",
    "for vocab in results_subcategory:\n",
    "    vocabulary_subcategory.extend(vocab)\n",
    "vocabulary_subcategory=list(set(vocabulary_subcategory))\n",
    "#    vocabulary.extend(vocab)\n",
    "#vocabulary=list(set(vocabulary))\n",
    "print(\"Updated Vocabulary: for subcategories\",vocabulary_subcategory)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3c7bd",
   "metadata": {},
   "source": [
    "# Step 1: Embedding con vettori binari 1/0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b749dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding culture representative samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#------------\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m#X_embed_train=embedding_sample(X_train,vocabulary)Ã¹\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmbedding culture representative samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m culture_representative_train=\u001b[43membedding_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m==\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcultural representative\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m).numpy()\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--Embedding culture agnostic sample\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m culture_agnostic_train=embedding_sample(X_train[X_train[:,\u001b[32m6\u001b[39m]==\u001b[33m'\u001b[39m\u001b[33mcultural agnostic\u001b[39m\u001b[33m'\u001b[39m],vocabulary,device).to(\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m).numpy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36membedding_sample\u001b[39m\u001b[34m(X, vocabulary, device)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m,X.shape[\u001b[32m0\u001b[39m]):\n\u001b[32m     14\u001b[39m     ent=extract_entity_id(X[j,\u001b[32m0\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     item_j = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43ment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     claim_item_i=item_j.data.get(\u001b[33m\"\u001b[39m\u001b[33mclaims\u001b[39m\u001b[33m\"\u001b[39m,{})\n\u001b[32m     17\u001b[39m     set_p=\u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Magistrale/ML_Homework/Homework_1/source/Libraries/lib/python3.12/site-packages/wikidata/client.py:148\u001b[39m, in \u001b[36mClient.get\u001b[39m\u001b[34m(self, entity_id, load)\u001b[39m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m.identity_map[entity_id] = entity\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[43mentity\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m entity\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Magistrale/ML_Homework/Homework_1/source/Libraries/lib/python3.12/site-packages/wikidata/entity.py:280\u001b[39m, in \u001b[36mEntity.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    279\u001b[39m url = \u001b[33m'\u001b[39m\u001b[33m./wiki/Special:EntityData/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.id)\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    282\u001b[39m     \u001b[38;5;28mself\u001b[39m.state = EntityState.non_existent\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Scrivania/Magistrale/ML_Homework/Homework_1/source/Libraries/lib/python3.12/site-packages/wikidata/client.py:202\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    200\u001b[39m logger.debug(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m: no cache; make a request...\u001b[39m\u001b[33m'\u001b[39m, url)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    204\u001b[39m     logger.debug(\u001b[33m'\u001b[39m\u001b[33mHTTP error code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m, e.code, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:515\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    512\u001b[39m     req = meth(req)\n\u001b[32m    514\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    518\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:532\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    531\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    491\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1392\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1344\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1342\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1343\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m         \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTransfer-encoding\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m   1347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1336\u001b[39m, in \u001b[36mHTTPConnection.request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body=\u001b[38;5;28;01mNone\u001b[39;00m, headers={}, *,\n\u001b[32m   1334\u001b[39m             encode_chunked=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1335\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1382\u001b[39m, in \u001b[36mHTTPConnection._send_request\u001b[39m\u001b[34m(self, method, url, body, headers, encode_chunked)\u001b[39m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(body, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1379\u001b[39m     \u001b[38;5;66;03m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[32m   1380\u001b[39m     \u001b[38;5;66;03m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[32m   1381\u001b[39m     body = _encode(body, \u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1331\u001b[39m, in \u001b[36mHTTPConnection.endheaders\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1091\u001b[39m, in \u001b[36mHTTPConnection._send_output\u001b[39m\u001b[34m(self, message_body, encode_chunked)\u001b[39m\n\u001b[32m   1089\u001b[39m msg = \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mself\u001b[39m._buffer)\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m._buffer[:]\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1094\u001b[39m \n\u001b[32m   1095\u001b[39m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[32m   1096\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[33m'\u001b[39m\u001b[33mread\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1097\u001b[39m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[32m   1098\u001b[39m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[32m   1099\u001b[39m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1035\u001b[39m, in \u001b[36mHTTPConnection.send\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1034\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_open:\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1477\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1474\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1475\u001b[39m     server_hostname = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m-> \u001b[39m\u001b[32m1477\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1478\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1042\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1039\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m:\n\u001b[32m   1040\u001b[39m                 \u001b[38;5;66;03m# non-blocking\u001b[39;00m\n\u001b[32m   1041\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1042\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1320\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout == \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m block:\n\u001b[32m   1319\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.settimeout(timeout)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# insert of binary vectors\n",
    "def gaussian_kernel_estimation(X,d):\n",
    "    sigma=1/(np.sqrt(2)*np.mean(d,axis=0))\n",
    "    dist_sq=np.sum((X-d)**2)\n",
    "    return np.exp(-dist_sq/(2*sigma**2))\n",
    "\n",
    "# too slow :(\n",
    "#----------\n",
    "def embedding_sample(X,vocabulary,device=None):\n",
    "    X_train_emb=t.zeros(X.shape[0],len(vocabulary),dtype=t.int,device=device)\n",
    "   # X_train_emb=np.zeros(shape=(X.shape[0],len(vocabulary)))\n",
    "    #print(f\"Processing {X.shape[0]} sample\")\n",
    "    for j in range(0,X.shape[0]):\n",
    "        ent=extract_entity_id(X[j,0])\n",
    "        item_j = client.get(ent, load=True)\n",
    "        claim_item_i=item_j.data.get(\"claims\",{})\n",
    "        set_p=set()\n",
    "        for prop_id,__ in islice(claim_item_i.items(),len(claim_item_i)):\n",
    "            set_p.add(str(prop_id))\n",
    "        #print(\"processed:\",claim_item_i)\n",
    "        for v in range(0,len(vocabulary)):\n",
    "            if vocabulary[v] in set_p:\n",
    "                X_train_emb[j,v]=1\n",
    "            else:\n",
    "                X_train_emb[j,v]=0\n",
    "        #np.arange(X_train_emb=X[j,6]\n",
    "        #print(f\"Update X_train_emb {j}: {X_train_emb[j]}\")\n",
    "    return X_train_emb\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "#------------\n",
    "#X_embed_train=embedding_sample(X_train,vocabulary)\n",
    "print(\"Embedding culture representative samples\")\n",
    "culture_representative_train=embedding_sample(X_train[X_train[:,6]=='cultural representative'],vocabulary,device).to('cpu').numpy()\n",
    "print(\"--Embedding culture agnostic sample\")\n",
    "culture_agnostic_train=embedding_sample(X_train[X_train[:,6]=='cultural agnostic'],vocabulary,device).to('cpu').numpy()\n",
    "print(\"--Embedding Culture exclusive sample\")\n",
    "culture_exclusive_train=embedding_sample(X_train[X_train[:,6]=='cultural exclusive'],vocabulary,device).to('cpu').numpy()\n",
    "\n",
    "\n",
    "print(f\"culture_agnostic_train: {culture_agnostic_train}\")\n",
    "print(f\"cutlure agnostic shape {culture_agnostic_train.shape}\")\n",
    "\n",
    "print(f\"culture_representative_train {culture_representative_train}\")\n",
    "print(f\"culture_representative_train shape {culture_representative_train.shape}\")\n",
    "\n",
    "print(f\"culture_exlusive_train {culture_exclusive_train}\")\n",
    "print(f\"culture_exlusive_train shape {culture_exclusive_train.shape}\")\n",
    "\n",
    "centroid_agnostic=np.mean(culture_agnostic_train,axis=0)\n",
    "print(f\"centroid agnostic for each property {centroid_agnostic.shape}\",centroid_agnostic)\n",
    "centroid_representative=np.mean(culture_representative_train,axis=0)\n",
    "print(f\"centroid_representative {centroid_representative.shape}\",centroid_representative)\n",
    "centroid_exclusive=np.mean(culture_exclusive_train,axis=0)\n",
    "print(f\"centroid_exclusive {centroid_exclusive.shape}\",centroid_exclusive)\n",
    "\n",
    "\n",
    "weights_agnostic=np.zeros(shape=len(vocabulary))\n",
    "weights_representative=np.zeros(shape=len(vocabulary))\n",
    "weights_exclusive=np.zeros(shape=len(len(vocabulary)))\n",
    "\n",
    "for i in range(len(vocabulary)):\n",
    "    weights_agnostic[i]=gaussian_kernel_estimation(X_train[0:,i],centroid_agnostic)\n",
    "    weights_representative[i]=gaussian_kernel_estimation(X_train[0:,i],centroid_representative)\n",
    "    weights_exclusive[i]=gaussian_kernel_estimation(X_train[0:,i],centroid_exclusive)\n",
    "\n",
    "# normalization\n",
    "weights_agnostic[i]/=np.sum(weights_agnostic)\n",
    "print(\"weights agnostic normalized for every property\")\n",
    "weights_representative[i]/=np.sum(weights_representative)\n",
    "weights_exclusive[i]/=np.sum(weights_exclusive)\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "# I'm arrived here\n",
    "def predict_entity_score(x_sample,centroid_CA,centroid_CR,centroid_CE,weights_agnostic,weights_representative,weights_exclusive):\n",
    "    similiraty_sample_CA=gaussian_kernel_estimation(x_sample,centroid_CA)\n",
    "    similarity_sample_CR=gaussian_kernel_estimation(x_sample,centroid_CR)\n",
    "    similarity_sample_CE=gaussian_kernel_estimation(x_sample,centroid_CE)\n",
    "    #weight_CA=\n",
    "    #weight_CR=\n",
    "   # weight_CE=\n",
    "    Sum_score_Agnostic=0\n",
    "    Sum_score_Representative=0\n",
    "    Sum_score_Exclusive=0\n",
    "    for i in range(0,x_sample.shape[0]):\n",
    "        Sum_score_Agnostic+=x_sample[i]*weights_agnostic[i]\n",
    "        Sum_score_Representative+=x_sample[i]*weights_representative[i]\n",
    "        Sum_score_Exclusive+=x_sample[i]*weights_exclusive[i]\n",
    "        \n",
    "    total_score_agnostic=Sum_score_Agnostic*similiraty_sample_CA\n",
    "    total_score_representative=Sum_score_Representative*similarity_sample_CR\n",
    "    total_score_exclusive=Sum_score_Agnostic*similarity_sample_CE\n",
    "    \n",
    "    return np.argmax([total_score_agnostic,total_score_representative,total_score_exclusive])\n",
    "\n",
    "#predict_entity_score()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Libraries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
