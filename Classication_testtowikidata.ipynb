{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e4a28c",
   "metadata": {},
   "source": [
    "# Classification /extraction \n",
    "--- \n",
    "\n",
    "we should consider only some properties derivated  from items but in this case we can build a vocabulary of property initially for two categories **Culture Representative** and **Culture Agnostic**. \n",
    "\n",
    "\n",
    "### How should we chose properties from item of Culture Agnostic and Culture Representative? \n",
    "### Training  Phase:\n",
    "\n",
    "--- \n",
    "\n",
    "### Step 1: Building 1 vocabulary of properties associated to Culture Representative and Culture Agnostic \n",
    "In this phase, for every sample $ item_{i} $ associated to, we put all its properties  into a set  $S_{P} $\n",
    "### Step 2: Embedded vectors of properties for every item \n",
    "In this case we associate a binary vector for every $item_{i} $ where for every possible property we associate 1 if it has that property $P_k$ otherwise 0.\n",
    "### Step 3: Compute the centroids for every category C.A C.R, C.E.X\n",
    "Now, after the preprocessed data we compute the centroid (like mean over every property $P_k$ on all samples) $C_{CA} $ and $C_{CR} $.\n",
    "### Step 4: We compute the euclidean distance among properties and centroids\n",
    "for every sample we compute distance among each property and centroids, so we will interpret strong relationship of some  properties wrt culture concepts while for concept more neutrals\n",
    "### Step 5: Corresponding the similarity to every property with some methods wrt centroids\n",
    "Every property, will have a weight computed following the importance of distance wrt centroids $C_{C.A} $ and  $ C_{C.R}$ : \n",
    "#### Case 1: Kernel Funtion (Kernel Density Estimation)\n",
    "we will use kernel to give a more flexible weight : $ w_{gauss}(item_{i})=\\exp{(-\\frac{d_{C}(item_{i})}{2*\\sigma^2})} $ .\n",
    "We can compute **$\\sigma$** like a constant such that influences the area of neighbours entities: \n",
    "we can compute it as: $\\sigma=\\frac{1}{\\sqrt{2}*mean \\space of \\space distance}$, can be optimized empirically \n",
    "**The Gaussian kernel has the advantage of providing a gradual decrease in weight rather than a linear or inversely proportional decrease, allowing us to assign greater weight to nearby entities without excluding those that are further away.** .\n",
    "We normalize all weights wrt the sum following weights wrt centroids\n",
    "\n",
    "### Test Phase:\n",
    "\n",
    "### Step 1: Compute distance among every element wrt to the both centroids\n",
    "we compute per every $item_i$  and every feature of kind: $P_{123},P_{2345} $ ecc... the euclidean distance\n",
    "### Step 2: Compute similarity for every test sample(with kernel approach) for both centroids\n",
    "this pass helps us to understand in which direction a sample should go to the centroids for both centroids \n",
    "### Step 3: Given averaged sum of the importance of feature of samples \n",
    "in this step we compute, Culture and Agnostic_score= $ \\sum_{fi=1}^{N_feat} item_{fi}* importance \\space of  \\space features $  the secnond term is measured in previous case\n",
    "### Step 4 : Then, to emphasize the influence of entities closer to the centroids, we multiply the weighted score by the similarity.\n",
    "So, ultimately, we multiply the weighted score by similarity to get a total score that is more sensitive to the entity's proximity to the centroids. This helps make a more accurate prediction based on how much the entity resembles the cultural or agnostic centers.\n",
    "\n",
    "total_culture_score=$ Culture \\space and  \\space Agnostic_score * similarity \\space of \\space distance \\space of \\space test_sample $ repeated for every class score\n",
    "total_agnostic_score = ... \n",
    "### Step 5: Classification based on best result\n",
    "if total_culture_score > total agnostic_score  -> **Culture Representative**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc8898",
   "metadata": {},
   "source": [
    "# Step 0: Building a Vocabulary with all properties of training items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae332ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca-colamarino/Documenti/GitHub/MNLP_HW1/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch as t \n",
    "import numpy as np\n",
    "from wikidata.client import Client\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bc816d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['http://www.wikidata.org/entity/Q32786' '916' '2012 film by M. Mohanan'\n",
      "  ... 'films' 'film' 'cultural exclusive']\n",
      " ['http://www.wikidata.org/entity/Q371' '!!!'\n",
      "  'American dance-punk band from California' ... 'music' 'musical group'\n",
      "  'cultural representative']\n",
      " ['http://www.wikidata.org/entity/Q3729947' 'Â¡Soborno!'\n",
      "  'Mort & Phil comic' ... 'comics and anime' 'comics'\n",
      "  'cultural representative']\n",
      " ...\n",
      " ['http://www.wikidata.org/entity/Q10779' 'Zwenkau'\n",
      "  'city in the district of Leipzig, in the Free State of Saxony, Germany'\n",
      "  ... 'geography' 'city' 'cultural exclusive']\n",
      " ['http://www.wikidata.org/entity/Q245296' 'zydeco'\n",
      "  'music genre evolved in southwest Louisiana which blends blues, rhythm and blues, and music indigenous to the Louisiana Creoles and the Native people of Louisiana'\n",
      "  ... 'music' 'music genre' 'cultural representative']\n",
      " ['http://www.wikidata.org/entity/Q129298' 'Zygmunt Chmielewski'\n",
      "  'actor (1894-1978)' ... 'performing arts' 'theatrical director'\n",
      "  'cultural exclusive']]\n"
     ]
    }
   ],
   "source": [
    "ds=load_dataset(\"sapienzanlp/nlp2025_hw1_cultural_dataset\")\n",
    "train_set=train_data=pd.DataFrame(ds[\"train\"])\n",
    "list_category=set(list(train_set.category))\n",
    "list_subcategory=set(list(train_set.category))\n",
    "X_train=train_set.values\n",
    "\n",
    "print(X_train) # stamp all dataset\n",
    "#print(X_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5fd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6251\n"
     ]
    }
   ],
   "source": [
    "def extract_entity_id(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "print(X_train.shape[0])\n",
    "def extract_sample_from_cat(X,cat):\n",
    "    l=list()\n",
    "    for elem in X:\n",
    "        if elem[4]==cat:\n",
    "            l.append(elem[0])\n",
    "    return np.array(l)\n",
    "#entity_train=np.zeros(shape=\n",
    "def count_frequency_prop_id(list_sampl):\n",
    "    counter=Counter()\n",
    "    for s in list_sampl:\n",
    "        for prop in s:\n",
    "            counter[prop]+=1\n",
    "    return counter\n",
    "\n",
    "# Implementation vocabulary on categories importance \n",
    "client = Client()\n",
    "vocabulary=list()\n",
    "def build_vocabulary(cat):\n",
    "    vocabulary_subset=list()\n",
    "    list_sample_cat=extract_sample_from_cat(X_train,cat)\n",
    "    #print(\"sample category: \"+cat)\n",
    "    #print(len(list_sample_cat))\n",
    "    set_properties=list()\n",
    "    #set_properties=np.array(set_properties)\n",
    "    for url in list_sample_cat:\n",
    "        entity_train=extract_entity_id(url)\n",
    "        item = client.get(entity_train, load=True)\n",
    "        claim_item_i=item.data.get(\"claims\",{})\n",
    "       # print(claim_item_i)\n",
    "        #print(len(claim_item_i))\n",
    "        set_property_item=set()\n",
    "        for prop_id,values in islice(claim_item_i.items(),len(claim_item_i)):\n",
    "            #prop_entity = client.get(prop_id, load=True)\n",
    "            #label = prop_entity.label\n",
    "            #print(f\"{prop_id} = {label} ({len(values)} statement{'s' if len(values) != 1 else ''})\")\n",
    "            set_property_item.add(str(prop_id))\n",
    "        set_properties.append(set_property_item)\n",
    "\n",
    "    print(\"Category:\"+cat+\"is collected\")\n",
    "    frequency_prop=count_frequency_prop_id(set_properties)\n",
    "    sorted_prop=frequency_prop.most_common()\n",
    "    print(sorted_prop)\n",
    "    #threshold=int(len(sorted_prop)*0.6)\n",
    "    # ERROR\n",
    "    #------\n",
    "    for prop_i,count in sorted_prop:\n",
    "        support_categories=count\n",
    "        if support_categories>=0.4:\n",
    "            vocabulary_subset.append(prop_i)\n",
    "    #print(\"Updated Vocabulary\",vocabulary_subset) \n",
    "    #-----       \n",
    "    return vocabulary_subset\n",
    "        #set_property_item.clear()\n",
    "    #print(\"intersection of sample belongs\"+cat,set.intersection(*set_properties))\n",
    "\n",
    "        #label = prop_entity.labels\n",
    "        #print(claim_item_i)\n",
    "with  ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results=list(executor.map(build_vocabulary,list_category))\n",
    "\n",
    "for vocab in results:\n",
    "    vocabulary.extend(vocab)\n",
    "vocabulary=list(set(vocabulary))\n",
    "print(\"Updated Vocabulary: for categories\",vocabulary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_id(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "print(X_train.shape[0])\n",
    "def extract_sample_from_cat(X,cat):\n",
    "    l=list()\n",
    "    for elem in X:\n",
    "        if elem[4]==cat:\n",
    "            l.append(elem[0])\n",
    "    return np.array(l)\n",
    "#entity_train=np.zeros(shape=\n",
    "def count_frequency_prop_id(list_sampl):\n",
    "    counter=Counter()\n",
    "    for s in list_sampl:\n",
    "        for prop in s:\n",
    "            counter[prop]+=1\n",
    "    return counter\n",
    "\n",
    "# Implementation vocabulary on categories importance \n",
    "client = Client()\n",
    "vocabulary=list()\n",
    "def build_vocabulary(cat):\n",
    "    vocabulary_subset=list()\n",
    "    list_sample_cat=extract_sample_from_cat(X_train,cat)\n",
    "    index = 1\n",
    "    lunghezza = len(list_sample_cat)\n",
    "    #print(\"sample category: \"+cat)\n",
    "    #print(len(list_sample_cat))\n",
    "    set_properties=list()\n",
    "    #set_properties=np.array(set_properties)\n",
    "    for url in list_sample_cat:\n",
    "        entity_train=extract_entity_id(url)\n",
    "        item = client.get(entity_train, load=True)\n",
    "        claim_item_i=item.data.get(\"claims\",{})\n",
    "       # print(claim_item_i)\n",
    "        #print(len(claim_item_i))\n",
    "        set_property_item=set()\n",
    "        for prop_id,values in islice(claim_item_i.items(),len(claim_item_i)):\n",
    "            #prop_entity = client.get(prop_id, load=True)\n",
    "            #label = prop_entity.label\n",
    "            #print(f\"{prop_id} = {label} ({len(values)} statement{'s' if len(values) != 1 else ''})\")\n",
    "            set_property_item.add(str(prop_id))\n",
    "        set_properties.append(set_property_item)\n",
    "        #print(cat+\"(\"+str(index)+\"/\"+str(lunghezza)+\" samples):\")\n",
    "        index=index+1\n",
    "        #print(set_properties)\n",
    "\n",
    "    tutte_proprieta = [prop for gruppo in set_properties for prop in gruppo]\n",
    "\n",
    "    # Conta le occorrenze\n",
    "    conteggio = Counter(tutte_proprieta)\n",
    "\n",
    "    # Ordina per frequenza decrescente e stampa\n",
    "    for prop, freq in conteggio.most_common():\n",
    "        print(f\"{prop}: {freq}\")\n",
    "\n",
    "with  ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    executor.map(build_vocabulary,list_subcategory)\n",
    "#\n",
    "#for vocab in results:\n",
    "#    vocabulary.extend(vocab)\n",
    "#vocabulary=list(set(vocabulary))\n",
    "#print(\"Updated Vocabulary: for categories\",vocabulary)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
