{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e4a28c",
   "metadata": {},
   "source": [
    "# Classification /extraction \n",
    "--- \n",
    "\n",
    "we should consider only some properties derivated  from items but in this case we can build a vocabulary of property initially for two categories **Culture Representative** and **Culture Agnostic**. \n",
    "\n",
    "\n",
    "### How should we chose properties from item of Culture Agnostic and Culture Representative? \n",
    "### Training  Phase:\n",
    "\n",
    "--- \n",
    "\n",
    "### Step 1: Building 1 vocabulary of properties associated to Culture Representative and Culture Agnostic \n",
    "In this phase, for every sample $ item_{i} $ associated to, we put all its properties  into a set  $S_{P} $\n",
    "### Step 2: Embedded vectors of properties for every item \n",
    "In this case we associate a binary vector for every $item_{i} $ where for every possible property we associate 1 if it has that property $P_k$ otherwise 0.\n",
    "### Step 3: Compute the centroids for every category C.A C.R, C.E.X\n",
    "Now, after the preprocessed data we compute the centroid (like mean over every property $P_k$ on all samples) $C_{CA} $ and $C_{CR} $.\n",
    "### Step 4: We compute the euclidean distance among properties and centroids\n",
    "for every sample we compute distance among each property and centroids, so we will interpret strong relationship of some  properties wrt culture concepts while for concept more neutrals\n",
    "### Step 5: Corresponding the similarity to every property with some methods wrt centroids\n",
    "Every property, will have a weight computed following the importance of distance wrt centroids $C_{C.A} $ and  $ C_{C.R}$ : \n",
    "#### Case 1: Kernel Funtion (Kernel Density Estimation)\n",
    "we will use kernel to give a more flexible weight : $ w_{gauss}(item_{i})=\\exp{(-\\frac{d_{C}(item_{i})}{2*\\sigma^2})} $ .\n",
    "We can compute **$\\sigma$** like a constant such that influences the area of neighbours entities: \n",
    "we can compute it as: $\\sigma=\\frac{1}{\\sqrt{2}*mean \\space of \\space distance}$, can be optimized empirically \n",
    "**The Gaussian kernel has the advantage of providing a gradual decrease in weight rather than a linear or inversely proportional decrease, allowing us to assign greater weight to nearby entities without excluding those that are further away.** .\n",
    "We normalize all weights wrt the sum following weights wrt centroids\n",
    "\n",
    "### Test Phase:\n",
    "\n",
    "### Step 1: Compute distance among every element wrt to the both centroids\n",
    "we compute per every $item_i$  and every feature of kind: $P_{123},P_{2345} $ ecc... the euclidean distance\n",
    "### Step 2: Compute similarity for every test sample(with kernel approach) for both centroids\n",
    "this pass helps us to understand in which direction a sample should go to the centroids for both centroids \n",
    "### Step 3: Given averaged sum of the importance of feature of samples \n",
    "in this step we compute, Culture and Agnostic_score= $ \\sum_{fi=1}^{N_feat} item_{fi}* importance \\space of  \\space features $  the secnond term is measured in previous case\n",
    "### Step 4 : Then, to emphasize the influence of entities closer to the centroids, we multiply the weighted score by the similarity.\n",
    "So, ultimately, we multiply the weighted score by similarity to get a total score that is more sensitive to the entity's proximity to the centroids. This helps make a more accurate prediction based on how much the entity resembles the cultural or agnostic centers.\n",
    "\n",
    "total_culture_score=$ Culture \\space and  \\space Agnostic_score * similarity \\space of \\space distance \\space of \\space test_sample $ repeated for every class score\n",
    "total_agnostic_score = ... \n",
    "### Step 5: Classification based on best result\n",
    "if total_culture_score > total agnostic_score  -> **Culture Representative**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fc8898",
   "metadata": {},
   "source": [
    "# Step 0: Building a Vocabulary with all properties of training items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae332ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch as t \n",
    "import numpy as np\n",
    "from wikidata.client import Client\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from scipy.spatial.distance import euclidean\n",
    "import urllib.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bc816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=load_dataset(\"sapienzanlp/nlp2025_hw1_cultural_dataset\")\n",
    "train_set=train_data=pd.DataFrame(ds[\"train\"])\n",
    "test_set=train_data=pd.DataFrame(ds[\"validation\"])\n",
    "list_category=set(list(train_set.category))\n",
    "print(list_category)\n",
    "list_subcategory=set(list(train_set.subcategory))\n",
    "X_train=train_set.values\n",
    "X_test =test_set.values\n",
    "\n",
    "print(X_train) # stamp all dataset\n",
    "#print(X_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_id(url):\n",
    "    return url.strip().split(\"/\")[-1]\n",
    "#print(X_train.shape[0])\n",
    "def extract_sample_from_cat(X,cat):\n",
    "    l=list()\n",
    "    for elem in X:\n",
    "        if elem[4]==cat:\n",
    "            l.append(elem[0])\n",
    "    return np.array(l)\n",
    "#entity_train=np.zeros(shape=\n",
    "#def count_frequency_prop_id(list_sampl):\n",
    "    \n",
    "\n",
    "def dynamic_threshold(n):\n",
    "    if n>=430:\n",
    "        return 0.35\n",
    "    elif n>=340:\n",
    "        return 0.4\n",
    "    elif n>=250:\n",
    "        return 0.45\n",
    "    else:\n",
    "        return 0.50\n",
    "# Implementation vocabulary on categories importance \n",
    "client = Client()\n",
    "vocabulary=list()\n",
    "def build_vocabulary(cat):\n",
    "    vocabulary_subset=list()\n",
    "    list_sample_cat=extract_sample_from_cat(X_train,cat)\n",
    "    number_list_sample_cat=len(list_sample_cat)\n",
    "    #print(\"sample category: \"+cat)\n",
    "    #print(len(list_sample_cat))\n",
    "    set_properties=list()\n",
    "    #set_properties=np.array(set_properties)\n",
    "    for url in list_sample_cat:\n",
    "        entity_train=extract_entity_id(url)\n",
    "        #-----Check on entity train\n",
    "        if not entity_train or not entity_train.startswith(\"Q\"): #verify if entity_id starts with Q\n",
    "            continue\n",
    "        try:\n",
    "            item = client.get(entity_train, load=True)\n",
    "        except urllib.error.HTTPError: #handle error of HTTP error like HTTP 404\n",
    "            continue\n",
    "        #------\n",
    "        claim_item_i=item.data.get(\"claims\",{})\n",
    "       # print(claim_item_i)\n",
    "        #print(len(claim_item_i))\n",
    "        set_property_item=set()\n",
    "        for prop_id,values in islice(claim_item_i.items(),len(claim_item_i)):\n",
    "            #prop_entity = client.get(prop_id, load=True)\n",
    "            #label = prop_entity.label\n",
    "            #print(f\"{prop_id} = {label} ({len(values)} statement{'s' if len(values) != 1 else ''})\")\n",
    "            set_property_item.add(str(prop_id))\n",
    "        set_properties.append(set_property_item)\n",
    "\n",
    "    print(\"Category:\"+cat+\" collected\")\n",
    "    counter=Counter()\n",
    "    for s in set_properties:\n",
    "        for prop in s:\n",
    "            counter[prop]+=1\n",
    "    frequency_prop=counter\n",
    "    #frequency_prop=count_frequency_prop_id(set_properties)\n",
    "    sorted_prop=frequency_prop.most_common()\n",
    "   # print(sorted_prop)\n",
    "    #threshold=int(len(sorted_prop)*0.6)\n",
    "    # ERROR\n",
    "    #------\n",
    "    th=0\n",
    "    if(number_list_sample_cat>=429):\n",
    "        th=0.35\n",
    "    elif number_list_sample_cat>=300:\n",
    "        th=0.4\n",
    "    elif number_list_sample_cat>=150:\n",
    "        th=0.45\n",
    "    #th=dynamic_threshold(number_list_sample_cat)\n",
    "    for prop_i,count in sorted_prop:\n",
    "        support_categories=count/number_list_sample_cat\n",
    "        if support_categories>=th: #min support \n",
    "            vocabulary_subset.append(prop_i)\n",
    "        #if len(vocabulary_subset)>=6: #the first top k\n",
    "            #break\n",
    "    #print(\"Updated Vocabulary\",vocabulary_subset) \n",
    "    #-----       \n",
    "    return vocabulary_subset\n",
    "        #set_property_item.clear()\n",
    "    #print(\"intersection of sample belongs\"+cat,set.intersection(*set_properties))\n",
    "\n",
    "        #label = prop_entity.labels\n",
    "        #print(claim_item_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a300f6",
   "metadata": {},
   "source": [
    "### Execute for category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with  ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results=list(executor.map(build_vocabulary,list_category))\n",
    "\n",
    "for vocab in results:\n",
    "    vocabulary.extend(vocab)\n",
    "vocabulary=list(set(vocabulary))\n",
    "print(\"Updated Vocabulary: for categories\",vocabulary)\n",
    "#Updated Vocabulary: for categories ['P495', 'P279', 'P345', 'P910', 'P17', 'P571', 'P373', 'P18', 'P625', 'P856', 'P244', 'P646', 'P31', 'P131', 'P641']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd090268",
   "metadata": {},
   "source": [
    "### Execute for subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with  ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results=list(executor.map(build_vocabulary,list_subcategory))\n",
    "\n",
    "for vocab in results:\n",
    "    vocabulary.extend(vocab)\n",
    "vocabulary=list(set(vocabulary))\n",
    "print(\"Updated Vocabulary: for categories\",vocabulary)\n",
    "#Updated Vocabulary: for categories ['P495', 'P279', 'P345', 'P910', 'P17', 'P571', 'P373', 'P18', 'P625', 'P856', 'P244', 'P646', 'P31', 'P131', 'P641']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3c7bd",
   "metadata": {},
   "source": [
    "# Step 1: Embedding con vettori binari 1/0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b749dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_estimation(X,d):\n",
    "    sigma=1/(np.sqrt(2)*np.mean(d,axis=0))\n",
    "    dist_sq=np.sum((X-d)**2)\n",
    "    return np.exp(-dist_sq/(2*sigma**2))\n",
    "\n",
    "# too slow :(\n",
    "#----------\n",
    "def embedding_sample(X,vocabulary,device=None):\n",
    "    X_train_emb=t.zeros(X.shape[0],len(vocabulary),dtype=t.int,device=device)\n",
    "   # X_train_emb=np.zeros(shape=(X.shape[0],len(vocabulary)))\n",
    "    #print(f\"Processing {X.shape[0]} sample\")\n",
    "    for j in range(0,X.shape[0]):\n",
    "        ent=extract_entity_id(X[j,0])\n",
    "        item_j = client.get(ent, load=True)\n",
    "        claim_item_i=item_j.data.get(\"claims\",{})\n",
    "        set_p=set()\n",
    "        for prop_id,__ in islice(claim_item_i.items(),len(claim_item_i)):\n",
    "            set_p.add(str(prop_id))\n",
    "        #print(\"processed:\",claim_item_i)\n",
    "        for v in range(0,len(vocabulary)):\n",
    "            if vocabulary[v] in set_p:\n",
    "                X_train_emb[j,v]=1\n",
    "            else:\n",
    "                X_train_emb[j,v]=0\n",
    "        #np.arange(X_train_emb=X[j,6]\n",
    "        #print(f\"Update X_train_emb {j}: {X_train_emb[j]}\")\n",
    "    return X_train_emb\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "#------------\n",
    "#X_embed_train=embedding_sample(X_train,vocabulary)\n",
    "print(\"Embedding culture representative samples\")\n",
    "culture_representative_train=embedding_sample(X_train[X_train[:,6]=='cultural representative'],vocabulary,device).to('cpu').numpy()\n",
    "print(\"--Embedding culture agnostic sample\")\n",
    "culture_agnostic_train=embedding_sample(X_train[X_train[:,6]=='cultural agnostic'],vocabulary,device).to('cpu').numpy()\n",
    "print(\"--Embedding Culture exclusive sample\")\n",
    "culture_exclusive_train=embedding_sample(X_train[X_train[:,6]=='cultural exclusive'],vocabulary,device).to('cpu').numpy()\n",
    "\n",
    "\n",
    "print(f\"culture_agnostic_train: {culture_agnostic_train}\")\n",
    "print(f\"cutlure agnostic shape {culture_agnostic_train.shape}\")\n",
    "\n",
    "print(f\"culture_representative_train {culture_representative_train}\")\n",
    "print(f\"culture_representative_train shape {culture_representative_train.shape}\")\n",
    "\n",
    "print(f\"culture_exlusive_train {culture_exclusive_train}\")\n",
    "print(f\"culture_exlusive_train shape {culture_exclusive_train.shape}\")\n",
    "\n",
    "centroid_agnostic=np.mean(culture_agnostic_train,axis=0)\n",
    "print(f\"centroid agnostic for each property {centroid_agnostic.shape}\",centroid_agnostic)\n",
    "centroid_representative=np.mean(culture_representative_train,axis=0)\n",
    "print(f\"centroid_representative {centroid_representative.shape}\",centroid_representative)\n",
    "centroid_exclusive=np.mean(culture_exclusive_train,axis=0)\n",
    "print(f\"centroid_exclusive {centroid_exclusive.shape}\",centroid_exclusive)\n",
    "\n",
    "\n",
    "weights_agnostic=np.zeros(shape=len(vocabulary))\n",
    "weights_representative=np.zeros(shape=len(vocabulary))\n",
    "weights_exclusive=np.zeros(shape=len(len(vocabulary)))\n",
    "\n",
    "for i in range(len(vocabulary)):\n",
    "    weights_agnostic[i]=gaussian_kernel_estimation(X_train[0:,i],centroid_agnostic)\n",
    "    weights_representative[i]=gaussian_kernel_estimation(X_train[0:,i],centroid_representative)\n",
    "    weights_exclusive[i]=gaussian_kernel_estimation(X_train[0:,i],centroid_exclusive)\n",
    "\n",
    "# normalization\n",
    "weights_agnostic[i]/=np.sum(weights_agnostic)\n",
    "print(\"weights agnostic normalized for every property\")\n",
    "weights_representative[i]/=np.sum(weights_representative)\n",
    "weights_exclusive[i]/=np.sum(weights_exclusive)\n",
    "\n",
    "\n",
    "#---\n",
    "\n",
    "# I'm arrived here\n",
    "def predict_entity_score(x_sample,centroid_CA,centroid_CR,centroid_CE,weights_agnostic,weights_representative,weights_exclusive):\n",
    "    similiraty_sample_CA=gaussian_kernel_estimation(x_sample,centroid_CA)\n",
    "    similarity_sample_CR=gaussian_kernel_estimation(x_sample,centroid_CR)\n",
    "    similarity_sample_CE=gaussian_kernel_estimation(x_sample,centroid_CE)\n",
    "    #weight_CA=\n",
    "    #weight_CR=\n",
    "   # weight_CE=\n",
    "    Sum_score_Agnostic=0\n",
    "    Sum_score_Representative=0\n",
    "    Sum_score_Exclusive=0\n",
    "    for i in range(0,x_sample.shape[0]):\n",
    "        Sum_score_Agnostic+=x_sample[i]*weights_agnostic[i]\n",
    "        Sum_score_Representative+=x_sample[i]*weights_representative[i]\n",
    "        Sum_score_Exclusive+=x_sample[i]*weights_exclusive[i]\n",
    "        \n",
    "    total_score_agnostic=Sum_score_Agnostic*similiraty_sample_CA\n",
    "    total_score_representative=Sum_score_Representative*similarity_sample_CR\n",
    "    total_score_exclusive=Sum_score_Agnostic*similarity_sample_CE\n",
    "    \n",
    "    return np.argmax([total_score_agnostic,total_score_representative,total_score_exclusive])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab2092",
   "metadata": {},
   "source": [
    "### implementazione gpu LUCA (Sicuro quella di Emilio è meglio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0810327a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding culture representative samples (GPU)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m start_time = time.time()\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEmbedding culture representative samples (GPU)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m culture_representative_train_gpu = \u001b[43membedding_sample_optimized_gpu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_numpy\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train_numpy\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcultural representative\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocabulary_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_entity_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--Embedding culture representative sample time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m start_time = time.time()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36membedding_sample_optimized_gpu\u001b[39m\u001b[34m(X_numpy, vocabulary, client, extract_entity_id, device)\u001b[39m\n\u001b[32m     17\u001b[39m ent = extract_entity_id(X_numpy[j, \u001b[32m0\u001b[39m])\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     item_j = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43ment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError: \u001b[38;5;66;03m#handle error of HTTP error like HTTP 404\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/MNLP_HW1/.venv/lib/python3.12/site-packages/wikidata/client.py:148\u001b[39m, in \u001b[36mClient.get\u001b[39m\u001b[34m(self, entity_id, load)\u001b[39m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m.identity_map[entity_id] = entity\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[43mentity\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m entity\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/MNLP_HW1/.venv/lib/python3.12/site-packages/wikidata/entity.py:280\u001b[39m, in \u001b[36mEntity.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    279\u001b[39m url = \u001b[33m'\u001b[39m\u001b[33m./wiki/Special:EntityData/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m.format(\u001b[38;5;28mself\u001b[39m.id)\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    282\u001b[39m     \u001b[38;5;28mself\u001b[39m.state = EntityState.non_existent\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/MNLP_HW1/.venv/lib/python3.12/site-packages/wikidata/client.py:202\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    200\u001b[39m logger.debug(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m: no cache; make a request...\u001b[39m\u001b[33m'\u001b[39m, url)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    204\u001b[39m     logger.debug(\u001b[33m'\u001b[39m\u001b[33mHTTP error code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m, e.code, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:515\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    512\u001b[39m     req = meth(req)\n\u001b[32m    514\u001b[39m sys.audit(\u001b[33m'\u001b[39m\u001b[33murllib.Request\u001b[39m\u001b[33m'\u001b[39m, req.full_url, req.data, req.headers, req.get_method())\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[32m    518\u001b[39m meth_name = protocol+\u001b[33m\"\u001b[39m\u001b[33m_response\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:532\u001b[39m, in \u001b[36mOpenerDirector._open\u001b[39m\u001b[34m(self, req, data)\u001b[39m\n\u001b[32m    529\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    531\u001b[39m protocol = req.type\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_open\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:492\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    490\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    491\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    494\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1392\u001b[39m, in \u001b[36mHTTPSHandler.https_open\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/urllib/request.py:1348\u001b[39m, in \u001b[36mAbstractHTTPHandler.do_open\u001b[39m\u001b[34m(self, http_class, req, **http_conn_args)\u001b[39m\n\u001b[32m   1346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[32m   1347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[32m-> \u001b[39m\u001b[32m1348\u001b[39m     r = \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1350\u001b[39m     h.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "import time\n",
    "\n",
    "\n",
    "# Configura il dispositivo\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def embedding_sample_optimized_gpu(X_numpy, vocabulary, client, extract_entity_id, device=None):\n",
    "    \"\"\"Crea embedding binari mantenendo il tensore sulla GPU.\"\"\"\n",
    "    num_samples = X_numpy.shape[0]\n",
    "    vocab_size = len(vocabulary)\n",
    "    X_train_emb = t.zeros((num_samples, vocab_size), dtype=t.float32, device=device)\n",
    "\n",
    "    for j in range(num_samples):\n",
    "        ent = extract_entity_id(X_numpy[j, 0])\n",
    "        try:\n",
    "            item_j = client.get(ent, load=True)\n",
    "        except urllib.error.HTTPError: #handle error of HTTP error like HTTP 404\n",
    "            continue\n",
    "        claim_item_i = item_j.data.get(\"claims\", {})\n",
    "        set_p = {str(prop_id) for prop_id in islice(claim_item_i.items(), len(claim_item_i))}\n",
    "        present_mask = t.tensor([vocab_item in set_p for vocab_item in vocabulary], dtype=t.bool, device=device)\n",
    "        X_train_emb[j, present_mask] = 1.0\n",
    "\n",
    "    return X_train_emb.int()\n",
    "def gaussian_kernel_estimation_torch(X, d, sigma):\n",
    "    \"\"\"Stima del kernel gaussiano usando tensori PyTorch.\"\"\"\n",
    "    dist_sq = t.sum((X - d)**2, dim=1, keepdim=True) if X.ndim > 1 else t.sum((X - d)**2)\n",
    "    return t.exp(-dist_sq / (2 * sigma**2))\n",
    "# Assumi che X_train e vocabulary siano già definiti come array NumPy\n",
    "X_train_numpy = X_train\n",
    "vocabulary_torch = [str(v) for v in vocabulary]\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Embedding culture representative samples (GPU)\")\n",
    "culture_representative_train_gpu = embedding_sample_optimized_gpu(\n",
    "    X_train_numpy[X_train_numpy[:, 6] == 'cultural representative'],\n",
    "    vocabulary_torch, client, extract_entity_id, device\n",
    ")\n",
    "print(f\"--Embedding culture representative sample time: {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"--Embedding culture agnostic sample (GPU)\")\n",
    "culture_agnostic_train_gpu = embedding_sample_optimized_gpu(\n",
    "    X_train_numpy[X_train_numpy[:, 6] == 'cultural agnostic'],\n",
    "    vocabulary_torch, client, extract_entity_id, device\n",
    ")\n",
    "print(f\"--Embedding culture agnostic sample time: {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"--Embedding Culture exclusive sample (GPU)\")\n",
    "culture_exclusive_train_gpu = embedding_sample_optimized_gpu(\n",
    "    X_train_numpy[X_train_numpy[:, 6] == 'cultural exclusive'],\n",
    "    vocabulary_torch, client, extract_entity_id, device\n",
    ")\n",
    "print(f\"--Embedding Culture exclusive sample time: {time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "print(f\"culture_agnostic_train_gpu: {culture_agnostic_train_gpu.cpu().numpy()}\")\n",
    "print(f\"cutlure agnostic shape {culture_agnostic_train_gpu.shape}\")\n",
    "print(f\"culture_representative_train_gpu {culture_representative_train_gpu.cpu().numpy()}\")\n",
    "print(f\"culture_representative_train_gpu shape {culture_representative_train_gpu.shape}\")\n",
    "print(f\"culture_exclusive_train_gpu {culture_exclusive_train_gpu.cpu().numpy()}\")\n",
    "print(f\"culture_exclusive_train_gpu shape {culture_exclusive_train_gpu.shape}\")\n",
    "\n",
    "# Calcolo dei centroidi sulla GPU\n",
    "centroid_agnostic_gpu = culture_agnostic_train_gpu.float().mean(dim=0)\n",
    "print(f\"centroid agnostic for each property (GPU) {centroid_agnostic_gpu.shape}\", centroid_agnostic_gpu.cpu().numpy())\n",
    "centroid_representative_gpu = culture_representative_train_gpu.float().mean(dim=0)\n",
    "print(f\"centroid_representative (GPU) {centroid_representative_gpu.shape}\", centroid_representative_gpu.cpu().numpy())\n",
    "centroid_exclusive_gpu = culture_exclusive_train_gpu.float().mean(dim=0)\n",
    "print(f\"centroid_exclusive (GPU) {centroid_exclusive_gpu.shape}\", centroid_exclusive_gpu.cpu().numpy())\n",
    "\n",
    "# Calcolo di sigma basato sui dati embeddati (ora su GPU)\n",
    "all_data_gpu = t.cat([\n",
    "    culture_agnostic_train_gpu.float(),\n",
    "    culture_representative_train_gpu.float(),\n",
    "    culture_exclusive_train_gpu.float()\n",
    "], dim=0)\n",
    "distances_gpu = t.cdist(all_data_gpu, all_data_gpu)\n",
    "sigma_gpu = 1 / (t.sqrt(t.tensor(2.0, device=device)) * (distances_gpu.mean() + 1e-8))\n",
    "\n",
    "# Calcolo dei pesi con il kernel gaussiano (ora sugli embedding sulla GPU)\n",
    "embeddings_agnostic_gpu = culture_agnostic_train_gpu.float()\n",
    "weights_agnostic_gpu = gaussian_kernel_estimation_torch(embeddings_agnostic_gpu, centroid_agnostic_gpu.unsqueeze(0), sigma_gpu).mean(dim=0)\n",
    "\n",
    "embeddings_representative_gpu = culture_representative_train_gpu.float()\n",
    "weights_representative_gpu = gaussian_kernel_estimation_torch(embeddings_representative_gpu, centroid_representative_gpu.unsqueeze(0), sigma_gpu).mean(dim=0)\n",
    "\n",
    "embeddings_exclusive_gpu = culture_exclusive_train_gpu.float()\n",
    "weights_exclusive_gpu = gaussian_kernel_estimation_torch(embeddings_exclusive_gpu, centroid_exclusive_gpu.unsqueeze(0), sigma_gpu).mean(dim=0)\n",
    "\n",
    "# Normalizzazione dei pesi (ora su GPU)\n",
    "weights_agnostic_gpu /= (weights_agnostic_gpu.sum() + 1e-8)\n",
    "print(\"weights agnostic normalized for every property (GPU)\")\n",
    "weights_representative_gpu /= (weights_representative_gpu.sum() + 1e-8)\n",
    "weights_exclusive_gpu /= (weights_exclusive_gpu.sum() + 1e-8)\n",
    "\n",
    "# Funzione di predizione ottimizzata per la GPU (gestisce input NumPy e lo sposta sulla GPU)\n",
    "def predict_entity_score_optimized_gpu(x_sample_np, centroid_CA_gpu, centroid_CR_gpu, centroid_CE_gpu,\n",
    "                                      weights_agnostic_gpu, weights_representative_gpu, weights_exclusive_gpu,\n",
    "                                      sigma_gpu):\n",
    "    \"\"\"Predice il punteggio di appartenenza usando tensori PyTorch sulla GPU.\"\"\"\n",
    "    x_sample = t.tensor(x_sample_np, dtype=t.float32, device=centroid_CA_gpu.device)\n",
    "\n",
    "    similarity_sample_CA = gaussian_kernel_estimation_torch(x_sample.unsqueeze(0), centroid_CA_gpu.unsqueeze(0), sigma_gpu)\n",
    "    similarity_sample_CR = gaussian_kernel_estimation_torch(x_sample.unsqueeze(0), centroid_CR_gpu.unsqueeze(0), sigma_gpu)\n",
    "    similarity_sample_CE = gaussian_kernel_estimation_torch(x_sample.unsqueeze(0), centroid_CE_gpu.unsqueeze(0), sigma_gpu)\n",
    "\n",
    "    weighted_sample_agnostic = x_sample * weights_agnostic_gpu\n",
    "    weighted_sample_representative = x_sample * weights_representative_gpu\n",
    "    weighted_sample_exclusive = x_sample * weights_exclusive_gpu\n",
    "\n",
    "    total_score_agnostic = weighted_sample_agnostic.sum() * similarity_sample_CA\n",
    "    total_score_representative = weighted_sample_representative.sum() * similarity_sample_CR\n",
    "    total_score_exclusive = weighted_sample_exclusive.sum() * similarity_sample_CE\n",
    "\n",
    "    return t.argmax(t.stack([total_score_agnostic, total_score_representative, total_score_exclusive])).item()\n",
    "\n",
    "# Esempio di predizione\n",
    "if X_train.shape[1] > len(vocabulary):\n",
    "    sample_to_predict = np.random.rand(len(vocabulary)) # Crea un campione casuale di embedding\n",
    "else:\n",
    "    sample_to_predict = np.random.rand(len(vocabulary)) # Genera un campione casuale se X_train non ha abbastanza colonne\n",
    "\n",
    "prediction = predict_entity_score_optimized_gpu(\n",
    "    sample_to_predict,\n",
    "    centroid_agnostic_gpu,\n",
    "    centroid_representative_gpu,\n",
    "    centroid_exclusive_gpu,\n",
    "    weights_agnostic_gpu,\n",
    "    weights_representative_gpu,\n",
    "    weights_exclusive_gpu,\n",
    "    sigma_gpu\n",
    ")\n",
    "print(f\"Prediction for sample: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
