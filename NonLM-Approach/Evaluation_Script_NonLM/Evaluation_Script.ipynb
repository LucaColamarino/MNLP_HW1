{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5f7d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "from scipy.spatial.distance import euclidean\n",
    "import json\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95ae85",
   "metadata": {},
   "source": [
    "## Load Test set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbc6fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_set=pd.read_csv(\"test_unlabeled.csv\")\n",
    "X_test_set=ds_test_set.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e7e13",
   "metadata": {},
   "source": [
    "## Load Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41dc6c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocabulary_category.json\",\"r\",encoding=\"utf-8\") as f: # open file vocabulary_category.json in read mode to load the stored vocabulary containing properties\n",
    "    vocabulary_j=json.load(f) # load vocabulary in json format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601bccc7",
   "metadata": {},
   "source": [
    "## Load tfidf to embedded sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f6573a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=joblib.load('tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace352cd",
   "metadata": {},
   "source": [
    "## Load Centroids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6234689",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids=np.load(\"centroids_every_label.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca59e273",
   "metadata": {},
   "source": [
    "## Function to return Original label predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a90a201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the original label associated to numerical label [0,1,2]\n",
    "def return_original_label(Y):\n",
    "    l_pred=[]\n",
    "    for y in Y:\n",
    "        if y==0:\n",
    "            l_pred.append(\"cultural agnostic\")\n",
    "        elif y==1:\n",
    "            l_pred.append(\"cultural representative\")\n",
    "        else:\n",
    "            l_pred.append(\"cultural exclusive\")\n",
    "    return l_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f14006",
   "metadata": {},
   "source": [
    "## Support Functions to predict samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e450242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the weight of a sample based on its distance from a centroid X2 (less distance->higher weights)\n",
    "def weight_distance_estimation(X1,X2):\n",
    "   \n",
    "    dist=euclidean(X1,X2) # computes euclidean distance\n",
    "    \n",
    "    weights=np.exp(-dist) # inverse of distance using an exponential trend for more robustness of weight values\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe77803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which extracts wiki-item Qxxx \n",
    "def extract_entity_id(url): \n",
    "    return url.strip().split(\"/\")[-1] \n",
    "#extract all samples from that class \n",
    "def extract_sample_from_class(X,label):\n",
    "    l=list()\n",
    "    for elem in X:\n",
    "        if elem[6]==label:\n",
    "            l.append(elem[0])\n",
    "    return l\n",
    "# Function to get request in a more efficient way without 1 to 1 corresponding request\n",
    "def handle_get_request(entities_list,labeled):\n",
    "        batch_size=40 # number of entity from extraction \n",
    "        url_base = \"https://www.wikidata.org/w/api.php\" # url base to peform a HTTP request to obtain Wikidata properties using api\n",
    "        results = {} # dictionary  where entity |Qxxx| are the keys while the claims will be the corresponding values\n",
    "        total_batches = (len(entities_list) + batch_size - 1) // batch_size # compute how many subsets of extraction \n",
    "\n",
    "        with tqdm(total=total_batches, desc=f\"Downloading batch of class {labeled}\") as pbar:\n",
    "            for i in range(0, len(entities_list), batch_size): # loop for every batch of all entities\n",
    "\n",
    "                batch = entities_list[i:i + batch_size] # i-th batch\n",
    "                ids_string = \"|\".join(batch) # Considers a sequence of |Q1xxx|Q2xxx| items to collect from the server\n",
    "                \n",
    "                # define parameters of get request\n",
    "                params = { \n",
    "                    \"action\": \"wbgetentities\", # obtain wbentities\n",
    "                    \"ids\": ids_string, # specify which items we want to extract corresponding to the i-th batch\n",
    "                    \"format\": \"json\"\n",
    "                }\n",
    "\n",
    "                attempt=0 # number of attempt in case of errors\n",
    "                success=False\n",
    "\n",
    "                while not success and attempt <3:\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        response_get_id = requests.get(url_base, params=params, timeout=20) # it obtains a get of properties using previous parameters  \n",
    "                        response_get_id.raise_for_status() # verify if the HTTP request fails and eventually store the error in e \n",
    "                        data = response_get_id.json() # extract data from json file\n",
    "\n",
    "                        entities = data.get(\"entities\", {}) # extract all properties of every item\n",
    "                        for entity_id, entity_data in entities.items():  \n",
    "                            claims = entity_data.get(\"claims\", {}) # obtain claims\n",
    "                            results[entity_id] = claims # collect all claims of an item |Qxxx| in a dictionary \n",
    "                        \n",
    "                        success=True\n",
    "                    except requests.exceptions.RequestException as e: \n",
    "\n",
    "                        print(f\"Batch Error {batch}: {e}\", flush=True)\n",
    "                        wait_time = 2 ** attempt  # we increase time with exponential control wrt the number of attempts\n",
    "                        \n",
    "                        print(f\"Retry waiting  {wait_time}s...\", flush=True)\n",
    "                        time.sleep(wait_time) # introduce a little execution delay to retry the same HTTP Request\n",
    "                        attempt+=1\n",
    "                pbar.update(1)\n",
    "            return results\n",
    "def extraction_identities_from_sample(X,C=None):\n",
    "    list_identities=[]\n",
    "    list_sample_cat=[]\n",
    "\n",
    "    # we detect 3 cases: \n",
    "    if type(C)==str: # case of explicit expression of a class :'cultural agnostic','cultural representative','cultural exclusive\n",
    "        \n",
    "        list_sample_cat=extract_sample_from_class(X,C) # extract all items from dataset belonging to class C\n",
    "\n",
    "    elif C!=None: # case of giving a list of categories\n",
    "\n",
    "        for elem in C:\n",
    "            list_sample_cat.extend(extract_sample_from_class(X,elem)) # we collect all url items \n",
    "    \n",
    "    else: # case of extraction of all samples without a specific category\n",
    "        \n",
    "        list_sample_cat=X[:,0]\n",
    "        \n",
    "    set_properties=list()\n",
    "\n",
    "    for url in list_sample_cat:\n",
    "\n",
    "        entity_train=extract_entity_id(url) # extract only the last part of url Qxxxx\n",
    "\n",
    "        if entity_train.startswith(\"Q\"): # verify if entity_id starts with Q\n",
    "            list_identities.append(entity_train)\n",
    "\n",
    "    claims_identity=handle_get_request(list_identities,C) # recall the function obtaining a dictionary of all properties associated to all selected items in list identities\n",
    "    \n",
    "\n",
    "    for entity_id, claims in claims_identity.items(): # take all claims from all entities\n",
    "\n",
    "        set_property_item=set()\n",
    "\n",
    "        for prop_id in claims.keys(): # take all properties Pxx from claims associated to each entitity\n",
    "            set_property_item.add(str(prop_id)) # create set of properties for that item \n",
    "\n",
    "        set_properties.append(set_property_item) # create a list of properties sets\n",
    "    return set_properties\n",
    "# extraction of identities from samples belonging to the same class \n",
    "def embedding_sample(X_data,vocabulary,sample_properties=None):\n",
    "    \n",
    "    def process_sample(X,vocab,set_p=None):\n",
    "        set_prop=extraction_identities_from_sample(X) # for every sample extract all identities\n",
    "        sample=np.zeros((X.shape[0],len(vocab)),dtype=np.int32) # create matrix of x_train_embedded (X_dimension,n_properties of vocabulary)\n",
    "\n",
    "        for i in range(0,len(set_prop)):\n",
    "            for v in range(0,len(vocab)):\n",
    "                if vocab[v] in set_prop[i]: # if property v belongs to the set_p of that sample -> sample cell at index v will be 1 \n",
    "                    sample[i,v]=1 \n",
    "        return sample\n",
    "    \n",
    "    return process_sample(X_data,vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5bb572",
   "metadata": {},
   "source": [
    "## Fucntion to predict sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49f2f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict function of every sample\n",
    "def predict_entity_score(x_sample,centroid_CA,centroid_CR,centroid_CE,T,tfidf):  \n",
    "\n",
    "    list_sample=[]\n",
    "    \n",
    "    # transform every sample associating weights wrt same distribution of train samples methodology\n",
    "    if T=='test':\n",
    "        x_sample=tfidf.transform([x_sample]).toarray()[0]\n",
    "        list_sample.append(x_sample)\n",
    "    \n",
    "    # compute the weights of every sample using the euclidean distance wrt specific centroid  \n",
    "    similiraty_sample_CA=weight_distance_estimation(x_sample,centroid_CA) \n",
    "    similarity_sample_CR=weight_distance_estimation(x_sample,centroid_CR)\n",
    "    similarity_sample_CE=weight_distance_estimation(x_sample,centroid_CE)\n",
    " \n",
    "    # compute the sum of all weights of similarity  \n",
    "    Sum_weight=similarity_sample_CE+similarity_sample_CR+similiraty_sample_CA\n",
    "\n",
    "    # normalize wrt this sum to obtain a value among [0,1] to treat as a probability\n",
    "    total_score_agnostic=similiraty_sample_CA/Sum_weight\n",
    "    total_score_exclusive=similarity_sample_CE/Sum_weight\n",
    "    total_score_representative=similarity_sample_CR/Sum_weight\n",
    "    \n",
    "    # build a vector with the following values and compute the argmax\n",
    "    v=[total_score_agnostic,total_score_representative,total_score_exclusive]\n",
    "    class_pred=np.argmax(v)\n",
    "    if T=='test':\n",
    "        return class_pred,list_sample\n",
    "    return class_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f2444",
   "metadata": {},
   "source": [
    "## Test Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2bdab6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5e674e41b44ac6890d567187fe3feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading batch of class None:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test_embed=embedding_sample(X_test_set,vocabulary_j)\n",
    "Y_test_pred=np.zeros(X_test_embed.shape[0],dtype=np.int32)\n",
    "list_x_test_weight=[]\n",
    "for i in range(0,X_test_embed.shape[0]):\n",
    "\n",
    "    # predict every validation sample with Centroid CA, Centroid CR, Centroid CE and corresponding property weights wrt CA,CR,CE\n",
    "    index_class_valid,list_sample_i=predict_entity_score(X_test_embed[i],centroids[\"centroid_CA\"],centroids[\"centroid_CR\"],centroids[\"centroid_CE\"],'test',tfidf=tfidf)\n",
    "    \n",
    "    # collect all y_pred\n",
    "    Y_test_pred[i]=index_class_valid \n",
    "\n",
    "    # collect all x_test_weigthted \n",
    "    list_x_test_weight.extend(list_sample_i)\n",
    "\n",
    "y_test=pd.DataFrame(np.column_stack((X_test_set[:,0],X_test_set[:,1],return_original_label(Y_test_pred))),columns=[\"item\",\"name\",\"label\"])\n",
    "y_test.to_csv(\"Salmonators_output_model_NONLM.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Libraries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
